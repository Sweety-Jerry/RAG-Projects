# -*- coding: utf-8 -*-
"""langchaintype.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uB25l1rB7vp06-ZeE4aFK58lZ1Y_pYwf
"""

pip install langchain_community

pip install langchain_groq

pip install faiss-cpu

from langchain_groq import ChatGroq
from langchain.chains import ConversationalRetrievalChain
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader
from langchain.memory import ConversationBufferMemory  # Or Summary/TokenBuffer

import os

# Step 1: Load documents
def load_docs(folder_path):
    docs = []
    for file in os.listdir(folder_path):
        if file.endswith(".txt"):
            loader = TextLoader(os.path.join(folder_path, file))
            docs.extend(loader.load())
    return docs

# Step 2: Split and embed
documents = load_docs("/content/sample_data/mynotes")  # Put your notes in this folder
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(documents)

embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embedding)

# Step 3: Retriever
retriever = vectorstore.as_retriever(search_type="similarity", k=3)

# Step 4: Memory â€” choose one
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer")
# memory = ConversationSummaryMemory(llm=llm, memory_key="chat_history")
# memory = ConversationTokenBufferMemory(llm=llm, memory_key="chat_history", max_token_limit=300)

# Step 5: LLM and Chain
llm = ChatGroq(temperature=0, model_name="llama3-8b-8192")

rag_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=True
)

# Step 6: Ask questions
while True:
    query = input("ðŸ’¬ You: ")
    if query.lower() in ["exit", "quit"]:
        break
    result = rag_chain.invoke({"question": query}) # Use invoke instead of run
    print("\nðŸ¤– Bot:", result["answer"]) # Access the answer key from the result dictionary